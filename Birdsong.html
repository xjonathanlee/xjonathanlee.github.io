<!DOCTYPE html>
<html lang="en">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Jonathan Lee - Dawn Chorus</title>

  <link href="css/main.css" rel="stylesheet">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.5.0/dist/lazyload.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/d3@7"></script>
<script src="https://cdn.jsdelivr.net/npm/@observablehq/plot@0.6"></script>
  <link href="https://fonts.googleapis.com/css?family=Roboto:300,700" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,700" rel="stylesheet">
<script src="js/followcursor.js"></script>
<link rel="icon" href="img/favicon.ico">

</head>

<body>

  <nav class="header-nav">
    <a href="index.html" class="nav-item">Home</a>
    <a href="about.html" class="nav-item">About</a>
  </nav>

<img class="full-bleed lazy" src="img/lazy-load.jpg" data-src="img/birdsong/birdsong1.gif" alt="gif of steveston" width="1080" height="1000">

  <section class="main-content">
    <h1 class="project-title">Dawn Chorus</h1>
    <p class="detail-intro">
      Recording nature has been done since the beginning of communication, we've needed this information recorded some type of <i>place</i>. In this design study I've attempted to illustrate and abstract the "dawn chorus" and the "evening chorus" of birds in my local area. 
    </p>

    <div class="sub-content-group">
      <ul class="sub-content-item">
        <li><em>Roles:</em>Interaction Design, Art Direction, Research Creation</li>
        <li><em>Tools:</em> Adobe Illustrator,Nature Journaling, Audio Identification, Data Synthesis</li>
        <li><em>Status:</em> 2022- Ongoing</li>
      </ul>
      <ul class="sub-content-item">
        <li><em>Context:</em> Personal Project</li>
         <li><em>Deliverables:</em> <a href="#">Bird Scarf</a></li>
      </ul>
    </div>

    <h2>Problem</h2>

    <p>Using Nature Journaling as a catalyst to create, how might we abstract the "Dawn Chorus"?</p>

    <h2>Design Process</h2>
    <p>I've been a nature photographer for about 3 years, and in order to catalogue new insights in the field I've taken up Nature Journaling during fleeting moments of quiet. Nature Journaling has opened my eyes to asking questions about the nature world and how we fit into this grand design. I wanted to look at bird song as a visual representation of place. This project started as a love letter to Giogia Lupi's "Dear Data" and over time morphed into something I cherish.</p> 

<!-- Demo1 -->
 <!--    <img class="full-bleed lazy full-screenimage" src="img/lazy-load.jpg" data-src="#" alt="#">
    <p class="annotate">A snapshot of my design process + sketches</p> -->

    
    <h2>Designing a Visual Language</h2>
    <p>Taking inspiration from Nature Journaling Techniques from the Audabon Society I encountered a method for catagorizing and recording bird song visually. From this I extrapolated a visual language based on call frequency, call length, and loudness based on location. Drawing from my musical background it was easy to see how we could add musical symbols to the "score".</p> 

    <p>Below you can see John Muirs Laws illustrating this principal: </p>

    <div class="videocontainer">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/3JsOf9CROCM?si=ceG38TtFu8FASWlv" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
    </div>

    <h2>Auditory Sight and Seeing</h2>
    <p>With birding and spatial audio, I developed a visual language to catagorize and catalogue bird numbers and estimations of size that fit the visual pattern. By picking apart observations at a granular scale during an "exposure" we can give users a visual sense of where and when birds come into the picture to paint a landscape. Birds can be counted through observation, and tracked to their behavior patterns based on species. While it is not an "accurate" portrayal, and you end up missing visual data, this visual system could be supportive of a robust information system as a catalogue (while also catering to birds not seen). The intent is to catalogue the data provided, but not take a picture.</p>
    
<!-- Demo1 -->
<!--     <img class="full-bleed lazy full-screenimage" src="img/lazy-load.jpg" data-src="#" alt="#">
    <p class="annotate">Visual Reference Guide</p> -->

    <h2>Data Management and Sound Forms</h2>

    <p>Bird Song as a medium is difficult to abstract because of its variance in sound texture and call length. In order to present a consistent aesthetic I used this paper(<a href="https://ui.adsabs.harvard.edu/abs/2002Natur.417..351B/abstract">https://ui.adsabs.harvard.edu/abs/2002Natur.417..351B/abstract</a>) as a metric to help reel in expectations of what an abstracted sound could look like. By diffrientiating bird song by length and by type (and by including a way to catagorize some birds as "unknown") this helped me create and define a color + texture pallette.</p>
        <img class="full-bleed lazy" src="img/lazy-load.jpg" data-src="img/birdsong/jp2.jpg" alt="#">
  <p class="annotate">Sound and Textures.</p>

<!-- Demo1 -->
<!--     <img class="full-bleed lazy full-screenimage" src="img/lazy-load.jpg" data-src="#" alt="#">
    <p class="annotate">How to read</p> -->

    <h2>Visual Impact</h2>

<!-- Maybe an abstract image here or a gif that shows interaction? -->

    <p>Initially I looked at the abstraction of birdsong similar to reading sheet music. Tying this together visually with observations I noticed that many bird songs operate in the same frequency range. To get around this challenge I used a visual overlay to group different birds together, reducing complexity and increasing readability. By linking several of the same "songs" together on the weave, we could isolate and observe patterns in different birds and calls over time.</p>

<!-- Slide Show of scanned sketches -->
<!--     <img class="full-bleed lazy" src="img/lazy-load.jpg" data-src="#" alt="#">
    <p class="annotate">Notebook Sketches</p> -->

    <img class="full-bleed lazy" src="img/lazy-load.jpg" data-src="img/birdsong/jp3.jpg" alt="#">
  <p class="annotate">Sound map exploration.</p>

    <p> By creating a way to illustrate time and direction as well as "intrusive songs" the goal was to create a sensory experience relative to timescale, allowing each observation to have its own "voice" musically. In subsequent exposures I wanted to create a way to identify sounds visually first by creating iconographs that correspond to the 3 taxa I've mapped sound into, this taxa was later simplified by shape instead of iconographs making the reading and translation process easier.</p>

    <img class="full-bleed lazy" src="img/lazy-load.jpg" data-src="img/birdsong/jp4.jpg" alt="#">
    <p class="annotate">Transcription of Sounds balancing asthetics with readability</p>


<!-- Demo1 -->
<!--     <img class="full-bleed lazy" src="img/lazy-load.jpg" data-src="#" alt="#">
    <p class="annotate">Explorations in design(2)</p> -->

    <h2> Functional vs Expressive </h2>
    <p>
      "Form following function" was the result of multiple iterations and studies of placement and scale. More specifically I wanted to look at how displaying the data aesthetically could lead to a better product.The hardest part here was finding a visual that showed cohesion and was bold enough to stand on its own. As I began to diverge from the initial idea of data formation, I explored other ideas that drew on pattern, repetition and more subtle encoding. 
    </p>
      <!-- Demo1 -->
<!--     <img class="full-bleed lazy full-screenimage" src="img/lazy-load.jpg" data-src="#" alt="#">
    <p class="annotate">Visual for maximum functionality.</p> -->

    <p>
      Expressing sound as a map was the result of multiple iterations and studies of readability and cohesion. More specifically I wanted to look at how displaying the data differently affected the perception of the piece. The hardest part was keeping a balance between readability and aesthetics, as the more "human" the data got, the more it limited readability. For this exploration I wanted to focus on the material of the scarf, exploring radial symmetry, sound cataloging and deep encoding of data.
    </p>

    <!-- Demo2 -->
<!--     <img class="full-bleed lazy full-screenimage" src="img/lazy-load.jpg" data-src="#" alt="#">
    <p class="annotate">Redrawing the visual for maximum expressiveness</p> -->
<h2>How to Read it</h2>
    <img class="full-bleed lazy full-screenimage" src="img/lazy-load.jpg" data-src="img/birdsong/how_read.png" alt="#">
    <p class="annotate">This is how the functional form is read.</p>

<p>
  The final product looked at each call as an individual bird signature, repeating a "capital call" for a distinct bird. Using scale and a grid system to drive the interaction. As the bird signature is "called" the song is logged in a column. Intrusive songs (Songs that are man made) are logged as well and compressed into the column. This column of bird + non bird songs is then repeated radially. Rough estimates of count are tracked using the eye system as well as a color chart to track location type and size using the a custom sizing guide. The grid system to the right of the eye icon illustrates the number of birds as a visual barometer using rough estimates and columns that reference guitar tabs. To minimize information overhead and to make the interaction cheifly about the picture it paints we've omitted bird type from the indicators, instead the colors of each song describe the "feel" or "flavour" of the sound quality.
</p>

  <p>I used radial symmetry as part of my designs. This enabled the scarf to be read in any orientation and also used the precedence set by conventional square scarfs. In the final product I demarcated the general orientation of readability as well as creating a way to have the user sightread the "auditory" landscape.</p>

    <h2> Final Product </h2>
    <!-- Demo1 -->
<img class="full-bleed lazy full-screenimage" src="img/lazy-load.jpg" data-src="img/birdsong/final_rough.png" alt="#">
    <p class="annotate">Final Result</p>




<!-- How to Read it -->
<!--     <img class="full-bleed lazy full-screenimage" src="img/lazy-load.jpg" data-src="#" alt="#">
    <p class="annotate">Final Result</p> -->

   <div class="footer-grid">
    <a  class="footerproject footer-col1" href="semalt.html">&larr; SIAR </a>
    <a  class="footerproject footer-col2" href="makePretty.html"> makePretty &rarr;</a>

</div>
  </section>
  <script src="js/lazyload.js"></script>
    <script src="js/intense.js"></script>
  <script type="text/javascript">
        console.log("BFBFBF");

      // Don't use window.onLoad like this in production, because it can only listen to one function.
      window.onload = function() {
        // Expose to window namespase for testing purposes
                console.log("nuignignin");
        var elements = document.querySelectorAll('.full-screenimage');
        Intense(elements);
      };

    window.requestAnimFrame = (function(){
  return  window.requestAnimationFrame       ||
          window.webkitRequestAnimationFrame ||
          window.mozRequestAnimationFrame    ||
          function( callback ){
            window.setTimeout(callback, 1000 / 60);
          };
})();

window.cancelRequestAnimFrame = ( function() {
    return window.cancelAnimationFrame          ||
        window.webkitCancelRequestAnimationFrame    ||
        window.mozCancelRequestAnimationFrame       ||
        window.oCancelRequestAnimationFrame     ||
        window.msCancelRequestAnimationFrame        ||
        clearTimeout
} )();


var Intense = (function() {

    'use strict';

    var KEYCODE_ESC = 27;

    // Track both the current and destination mouse coordinates
    // Destination coordinates are non-eased actual mouse coordinates
    var mouse = { xCurr:0, yCurr:0, xDest: 0, yDest: 0 };

    var horizontalOrientation = true;

    // Holds the animation frame id.
    var looper;
  
    // Current position of scrolly element
    var lastPosition, currentPosition = 0;
    
    var sourceDimensions, target;
    var targetDimensions = { w: 0, h: 0 };
  
    var container;
    var containerDimensions = { w: 0, h:0 };
    var overflowArea = { x: 0, y: 0 };

    // Overflow variable before screen is locked.
    var overflowValue;

    /* -------------------------
    /*          UTILS
    /* -------------------------*/

    // Soft object augmentation
    function extend( target, source ) {

        for ( var key in source )

            if ( !( key in target ) )

                target[ key ] = source[ key ];

        return target;
    }

    // Applys a dict of css properties to an element
    function applyProperties( target, properties ) {

      for( var key in properties ) {
        target.style[ key ] = properties[ key ];
      }
    }

    // Returns whether target a vertical or horizontal fit in the page.
    // As well as the right fitting width/height of the image.
    function getFit( 

      source ) {

      var heightRatio = window.innerHeight / source.h;

      if( (source.w * heightRatio) > window.innerWidth ) {
        return { w: source.w * heightRatio, h: source.h * heightRatio, fit: true };
      } else {
        var widthRatio = window.innerWidth / source.w;
        return { w: source.w * widthRatio, h: source.h * widthRatio, fit: false };
      }
    }

    /* -------------------------
    /*          APP
    /* -------------------------*/

    function startTracking( passedElements ) {

      var i;

      // If passed an array of elements, assign tracking to all.
      if ( passedElements.length ) {

        // Loop and assign
        for( i = 0; i < passedElements.length; i++ ) {
          track( passedElements[ i ] );
        }

      } else {
          track( passedElements );
      }
    }

    function track( element ) {

      // Element needs a src at minumun.
      if( element.getAttribute( 'data-image') || element.src ) {
        element.addEventListener( 'click', function() {
          init( this );
        }, false );
      }
    }
  
    function start() { 
      loop();
    }
   
    function stop() {
      cancelRequestAnimFrame( looper );
    }

    function loop() {
        looper = requestAnimFrame(loop);
        positionTarget();      
    }

    // Lock scroll on the document body.
    function lockBody() {

      overflowValue = document.body.style.overflow;
      document.body.style.overflow = 'hidden';
    }

    // Unlock scroll on the document body.
    function unlockBody() {
      document.body.style.overflow = overflowValue;
    }

    function createViewer( title, caption ) {

      /*
       *  Container
       */
      var containerProperties = {
        'backgroundColor': 'rgba(0,0,0,0.8)',
        'width': '100%',
        'height': '100%',
        'position': 'fixed',
        'top': '0px',
        'left': '0px',
        'overflow': 'hidden',
        'zIndex': '999999',
        'margin': '0px',
        'webkitTransition': 'opacity 150ms cubic-bezier( 0, 0, .26, 1 )',
        'MozTransition': 'opacity 150ms cubic-bezier( 0, 0, .26, 1 )',
        'transition': 'opacity 150ms cubic-bezier( 0, 0, .26, 1 )',
        'opacity': '0'
      }
      container = document.createElement( 'figure' );
      container.appendChild( target );
      applyProperties( container, containerProperties );
// base64 hexcode for the ex_closeimg
      var imageProperties = {
        'cursor': 'url( "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAMAAAAp4XiDAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAADNQTFRFrKurTElKMS0ukY+PWldYnp2d4+PjyMfHdnN0urm51tXVaGVmPzs8hIGC8fHxIx8g////czzPnwAAABF0Uk5T/////////////////////wAlrZliAAAB90lEQVR42pSWWaKFIAhAydnqGvtf7RPNHMLq8XMHPQgIAuBdVpAqiYSVWYbxD+/E0Yhw/hkJcj9ussswR3Q+QESLbJRo3/mHniDrki2x7brNdi4rhwCticGIZGxagDuiSdkvMBHC8KM1PSKONFmciKWDXI8A2WtwKob8hBbxRAR8kECMr4gRb0RmhLkQFX+t+CJr1KsK4q8zH6VsIyQmyYYfZIvJkxGKlvmCmBw1SJ44/CQueQOJNd+QvBUoU1Sf3DXeYemLQ1HeQP5oiXpHdBd9dZB6wNEuuDIhNFlSLUOw8RqHQj2ZO4EYr9OCH10pDEeQFx5krBLkGI7AWDmSEIkcwxEop0iuUY3/QJIfXD1MkUxwTEL0PY1zrIBjYjJruhfFEe2ddkG2kC6UI1iGUgWowOyQ4Ufz7nR1YanIgLb8hlOg3k9/yo9UAJV0n2SheQd8GFPMU1UK7ppZgaQdkvH7N2RPrsH1Cnw6hEoLcoxEeCeCyPGD8/v26RlLmqHkrX4jrk1QFLy5Q45sTbNgixb5R6S0JGoXXKk1SX+2itr4qBUcavJqGnU07QS6BGYacmnJNdmqA8Glti8NBxwusNOFz9PIpq+OtuotzyR+NpCcGskMGpSWMvr05g6RDXoZp55Fh5fhioy5xivRGPmA5IpNwq/9CTAANk+MGc+qemwAAAAASUVORK5CYII=" ) 25 25, auto'
      }
      applyProperties( target, imageProperties );

      /*
       *  Caption Container
       */
      var captionContainerProperties = {
        'fontFamily': 'Georgia, Times, "Times New Roman", serif',
        'position': 'fixed',
        'bottom': '0px',
        'left': '0px',
        'padding': '20px',
        'color': '#fff',
        'wordSpacing': '0.2px',
        'webkitFontSmoothing': 'antialiased',
        'textShadow': '-1px 0px 1px rgba(0,0,0,0.4)'
      }
      var captionContainer = document.createElement( 'figcaption' );
      applyProperties( captionContainer, captionContainerProperties );

      /*
       *  Caption Title
       */
      if ( title ) {
        var captionTitleProperties = {
          'margin': '0px',
          'padding': '0px',
          'fontWeight': 'normal',
          'fontSize': '40px',
          'letterSpacing': '0.5px',
          'lineHeight': '35px',
          'textAlign': 'left'
        }
        var captionTitle = document.createElement( 'h1' );
        applyProperties( captionTitle, captionTitleProperties );
        captionTitle.innerHTML = title;
        captionContainer.appendChild( captionTitle );
      }

      if ( caption ) {
        var captionTextProperties = {
          'margin': '0px',
          'padding': '0px',
          'fontWeight': 'normal',
          'fontSize': '20px',
          'letterSpacing': '0.1px',
          'maxWidth': '500px',
          'textAlign': 'left',
          'background': 'none',
          'marginTop': '5px'
        }
        var captionText = document.createElement( 'h2' );
        applyProperties( captionText, captionTextProperties );
        captionText.innerHTML = caption;
        captionContainer.appendChild( captionText );
      }

      container.appendChild( captionContainer );

      setDimensions();

      mouse.xCurr = mouse.xDest = window.innerWidth / 2;
      mouse.yCurr = mouse.yDest = window.innerHeight / 2;
      
      document.body.appendChild( container );
      setTimeout( function() {
        container.style[ 'opacity' ] = '1';
      }, 10);
    }

    function removeViewer() {

      unlockBody();
      unbindEvents();
      document.body.removeChild( container );
    }

    function setDimensions() {

      // Manually set height to stop bug where 
      var imageDimensions = getFit( sourceDimensions );
      target.width = imageDimensions.w;
      target.height = imageDimensions.h;
      horizontalOrientation = imageDimensions.fit;

      targetDimensions = { w: target.width, h: target.height };
      containerDimensions = { w: window.innerWidth, h: window.innerHeight };
      overflowArea = {x: containerDimensions.w - targetDimensions.w, y: containerDimensions.h - targetDimensions.h};

    }

    function init( element ) {

      var imageSource = element.getAttribute( 'data-image') || element.src;
      var title = element.getAttribute( 'data-title');
      var caption = element.getAttribute( 'data-caption');
      
      var img = new Image();
      img.onload = function() {

        sourceDimensions = { w: img.width, h: img.height }; // Save original dimensions for later.
        target = this;
        createViewer( title, caption );
        lockBody();
        bindEvents();
        loop();
      }

      img.src = imageSource;
    }

    function bindEvents() {

      container.addEventListener( 'mousemove', onMouseMove,   false );
      container.addEventListener( 'touchmove', onTouchMove,   false );
      window.addEventListener(    'resize',    setDimensions, false );
      window.addEventListener(    'keyup',     onKeyUp,       false );
      target.addEventListener(    'click',     removeViewer,  false );
    }

    function unbindEvents() {

      container.removeEventListener( 'mousemove', onMouseMove,   false );
      container.removeEventListener( 'touchmove', onTouchMove,   false);
      window.removeEventListener(    'resize',    setDimensions, false );
      window.removeEventListener(    'keyup',     onKeyUp,       false );
      target.removeEventListener(    'click',     removeViewer,  false )
    }
  
    function onMouseMove( event ) {

      mouse.xDest = event.clientX;
      mouse.yDest = event.clientY;
    }

    function onTouchMove( event ) {

      event.preventDefault(); // Needed to keep this event firing.
      mouse.xDest = event.touches[0].clientX;
      mouse.yDest = event.touches[0].clientY;
    }

    // Exit on excape key pressed;
    function onKeyUp( event ) {

      event.preventDefault();
      if ( event.keyCode === KEYCODE_ESC ) {
        removeViewer();
      } 
    }
  
    function positionTarget() {

      mouse.xCurr += ( mouse.xDest - mouse.xCurr ) * 0.05;
      mouse.yCurr += ( mouse.yDest - mouse.yCurr ) * 0.05;

      if ( horizontalOrientation === true ) {

        // HORIZONTAL SCANNING
        currentPosition += ( mouse.xCurr - currentPosition );
        if( mouse.xCurr !== lastPosition ) {
          var position = parseFloat( currentPosition / containerDimensions.w );
          position = overflowArea.x * position;
          target.style[ 'webkitTransform' ] = 'translate3d(' + position + 'px, 0px, 0px)';
          target.style[ 'MozTransform' ] = 'translate3d(' + position + 'px, 0px, 0px)';
          target.style[ 'msTransform' ] = 'translate3d(' + position + 'px, 0px, 0px)';
          lastPosition = mouse.xCurr;
        }
      } else if ( horizontalOrientation === false ) {

        // VERTICAL SCANNING
        currentPosition += ( mouse.yCurr - currentPosition );
        if( mouse.yCurr !== lastPosition ) {
          var position = parseFloat( currentPosition / containerDimensions.h );
          position = overflowArea.y * position;
          target.style[ 'webkitTransform' ] = 'translate3d( 0px, ' + position + 'px, 0px)';
          target.style[ 'MozTransform' ] = 'translate3d( 0px, ' + position + 'px, 0px)';
          target.style[ 'msTransform' ] = 'translate3d( 0px, ' + position + 'px, 0px)';
          lastPosition = mouse.yCurr;
        }
      }
    }

    function main( element ) {

      // Parse arguments

      if ( !element ) {
        throw 'You need to pass an element!';
      }

      startTracking( element );
    }

    return extend( main, {
        resize: setDimensions,
        start: start,
        stop: stop
    });

})();
</script>
</body>